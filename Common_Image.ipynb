{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GTKZfing21yp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import numpy as np\n",
        "import PIL.Image as img\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "import tensorflow as tf\n",
        "\n",
        "# Option\n",
        "mode = 'Training'\n",
        "num_cluster = 153\n",
        "eps = 1e-10\n",
        "height = 300\n",
        "width = 300\n",
        "channel = 3\n",
        "\n",
        "def img_array(path):\n",
        "    image = img.open(path)\n",
        "    tmp = np.array(image)\n",
        "    image.close()\n",
        "    return tmp\n",
        "\n",
        "# Regex\n",
        "re_model = re.compile(\"^(\\d+)_\")\n",
        "\n",
        "train = glob.glob('train/*.jpg')\n",
        "train_path = os.listdir('train')\n",
        "train_img_paths = [filename for filename in train_path if filename.endswith('jpg')]\n",
        "\n",
        "# Image Resizing (HeightxWeightxChannel) and Get numpy array\n",
        "train_X = np.array([np.array(img.open(fname).resize((height, width), img.ANTIALIAS)) for fname in train])\n",
        "train_Y = np.array([re_model.match(img_path).group(1) for img_path in train_img_paths])\n",
        "print('train load finished')\n",
        "\n",
        "test = glob.glob('test/*.jpg')\n",
        "test_path = os.listdir('test')\n",
        "test_img_paths = [filename for filename in test_path if filename.endswith('jpg')]\n",
        "\n",
        "# Image Resizing (HeightxWeightxChannel) and Get numpy array\n",
        "test_X = np.array([np.array(img.open(fname).resize((height, width), img.ANTIALIAS)) for fname in test])\n",
        "test_Y = np.array([re_model.match(img_path).group(1) for img_path in test_img_paths])\n",
        "print('test load finished')\n",
        "\n",
        "# Get Datas and Labels as batch_size\n",
        "def get_batch(batch_size, img_data, imgt_labels):\n",
        "    batch_index = random.sample(range(len(imgt_labels)), batch_size)\n",
        "\n",
        "    batch_data = np.empty([batch_size, height, width, channel], dtype=np.float32)\n",
        "    batch_label = np.empty([batch_size], dtype=np.int32)\n",
        "\n",
        "    for n, i in enumerate(batch_index):\n",
        "        batch_data[n, ...] = img_data[i, ...]\n",
        "        batch_label[n] = imgt_labels[i]\n",
        "    return batch_data, batch_label\n",
        "\n",
        "# Get Datas and Labels as batch_size for Testing\n",
        "def get_batch_test(batch_size, img_data, i):\n",
        "    batch_data = np.copy(img_data[batch_size * i:batch_size * (i + 1), ...])\n",
        "    return batch_data\n",
        "\n",
        "def clustering_acc(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    ind = linear_assignment(w.max() - w)\n",
        "\n",
        "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
        "\n",
        "def NMI(y_true, y_pred):\n",
        "    return metrics.normalized_mutual_info_score(y_true, y_pred)\n",
        "\n",
        "def ARI(y_true, y_pred):\n",
        "    return metrics.adjusted_rand_score(y_true, y_pred)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "def ConvNetwork(in_img, num_cluster, name='ConvNetwork', reuse=False):\n",
        "    with tf.variable_scope(name, reuse=reuse):\n",
        "        # 64 x 3 -> pooling\n",
        "        conv1 = tf.layers.conv2d(in_img, 64, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv1 = tf.layers.batch_normalization(conv1, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv1 = tf.nn.relu(conv1)\n",
        "        conv2 = tf.layers.conv2d(conv1, 64, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv2 = tf.layers.batch_normalization(conv2, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv2 = tf.nn.relu(conv2)\n",
        "        conv3 = tf.layers.conv2d(conv2, 64, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv3 = tf.layers.batch_normalization(conv3, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv3 = tf.nn.relu(conv3)\n",
        "        conv3 = tf.layers.max_pooling2d(conv3, [2, 2], [2, 2])\n",
        "        conv3 = tf.layers.batch_normalization(conv3, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\n",
        "        # 128 x 3 -> pooling\n",
        "        conv4 = tf.layers.conv2d(conv3, 128, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv4 = tf.layers.batch_normalization(conv4, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv4 = tf.nn.relu(conv4)\n",
        "        conv5 = tf.layers.conv2d(conv4, 128, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv5 = tf.layers.batch_normalization(conv5, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv5 = tf.nn.relu(conv5)\n",
        "        conv6 = tf.layers.conv2d(conv5, 128, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv6 = tf.layers.batch_normalization(conv6, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv6 = tf.nn.relu(conv6)\n",
        "        conv6 = tf.layers.max_pooling2d(conv6, [2, 2], [2, 2])\n",
        "        conv6 = tf.layers.batch_normalization(conv6, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\n",
        "        # 256 x 3 -> pooling\n",
        "        conv7 = tf.layers.conv2d(conv6, 256, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv7 = tf.layers.batch_normalization(conv7, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv7 = tf.nn.relu(conv7)\n",
        "        conv8 = tf.layers.conv2d(conv7, 256, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv8 = tf.layers.batch_normalization(conv8, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv8 = tf.nn.relu(conv8)\n",
        "        conv9 = tf.layers.conv2d(conv8, 256, [3, 3], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        conv9 = tf.layers.batch_normalization(conv9, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        conv9 = tf.nn.relu(conv9)\n",
        "        conv9 = tf.layers.max_pooling2d(conv9, [2, 2], [2, 2])\n",
        "        conv9 = tf.layers.batch_normalization(conv9, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\n",
        "        # 1x1 kernel\n",
        "        one = tf.layers.conv2d(conv9, num_cluster, [1, 1], [1, 1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "        one = tf.layers.batch_normalization(one, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        one = tf.nn.relu(one)\n",
        "        one = tf.layers.average_pooling2d(one, [6, 6], [2, 2])\n",
        "        flat = tf.layers.flatten(one)\n",
        "\n",
        "        # two Full-Connected Layer\n",
        "        # dense 0\n",
        "        x0 = tf.layers.dense(flat, num_cluster, kernel_initializer=tf.initializers.identity())\n",
        "        x0 = tf.layers.batch_normalization(x0, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        x0 = tf.nn.relu(x0)\n",
        "        # dense 1\n",
        "        x1 = tf.layers.dense(x0, num_cluster, kernel_initializer=tf.initializers.identity())\n",
        "        x1 = tf.layers.batch_normalization(x1, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "        x1 = tf.nn.relu(x1)\n",
        "\n",
        "        out = tf.nn.softmax(x1)\n",
        "\n",
        "    return out\n",
        "\n",
        "image_pool_input = tf.placeholder(shape=[None, height, width, channel], dtype=tf.float32, name='image_pool_input')\n",
        "u_thres = tf.placeholder(shape=[], dtype=tf.float32, name='u_thres')\n",
        "l_thres = tf.placeholder(shape=[], dtype=tf.float32, name='l_thres')\n",
        "lr = tf.placeholder(shape=[], dtype=tf.float32, name='learning_rate')\n",
        "\n",
        "label_feat = ConvNetwork(image_pool_input, num_cluster, name='ConvNetwork', reuse=False)\n",
        "label_feat_norm = tf.nn.l2_normalize(label_feat, dim=1)\n",
        "sim_mat = tf.matmul(label_feat_norm, label_feat_norm, transpose_b=True)\n",
        "\n",
        "pos_loc = tf.greater(sim_mat, u_thres, name='greater')\n",
        "neg_loc = tf.less(sim_mat, l_thres, name='less')\n",
        "pos_loc_mask = tf.cast(pos_loc, dtype=tf.float32)\n",
        "neg_loc_mask = tf.cast(neg_loc, dtype=tf.float32)\n",
        "\n",
        "pred_label = tf.argmax(label_feat, axis=1)\n",
        "\n",
        "# Deep Adaptive Image Clustering Cost Function Optimize\n",
        "pos_entropy = tf.multiply(-tf.log(tf.clip_by_value(sim_mat, eps, 1.0)), pos_loc_mask)\n",
        "neg_entropy = tf.multiply(-tf.log(tf.clip_by_value(1-sim_mat, eps, 1.0)), neg_loc_mask)\n",
        "\n",
        "loss_sum = tf.reduce_mean(pos_entropy) + tf.reduce_mean(neg_entropy)\n",
        "train_op = tf.train.RMSPropOptimizer(lr).minimize(loss_sum)\n",
        "\n",
        "image_data = np.concatenate([train_X, test_X], axis=0)\n",
        "image_label = np.concatenate([train_Y, test_Y], axis=0)\n",
        "\n",
        "print(len(image_data))\n",
        "\n",
        "# Compress Dimension\n",
        "mapping = {}\n",
        "mapped_label = 0\n",
        "for index,data in enumerate(image_label):\n",
        "  if(data in mapping):\n",
        "    image_label[index] = mapping[data]\n",
        "  else:\n",
        "    image_label[index] = mapping[data] = mapped_label+1\n",
        "    mapped_label += 1\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "if mode == 'Training':\n",
        "    batch_size = 12\n",
        "    test_batch_size = 64\n",
        "    base_lr = 0.001\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        lamda = 0\n",
        "        epoch = 1\n",
        "        u = 0.95\n",
        "        l = 0.455\n",
        "\n",
        "        while u > l:\n",
        "            u = 0.95 - lamda\n",
        "            l = 0.455 + 0.1 * lamda\n",
        "            print(u, l)\n",
        "            for i in range(1, int(1001)):  # 1000 iterations is roughly 1 epoch\n",
        "                data_samples, _ = get_batch(batch_size, image_data, image_label)\n",
        "                feed_dict = {image_pool_input: data_samples, u_thres: u, l_thres: l, lr: base_lr}\n",
        "                train_loss, _ = sess.run([loss_sum, train_op], feed_dict=feed_dict)\n",
        "                if i % 20 == 0:\n",
        "                    print('training loss at iter %d is %f' % (i, train_loss))\n",
        "\n",
        "            lamda += 1.1 * 0.009\n",
        "            print(lamda)\n",
        "            # run testing every epoch\n",
        "            data_samples, data_labels = get_batch(test_batch_size, image_data, image_label)\n",
        "            feed_dict = {image_pool_input: data_samples}\n",
        "            pred_cluster = sess.run(pred_label, feed_dict=feed_dict)\n",
        "\n",
        "            acc = c(data_labels, pred_cluster)\n",
        "            nmi = NMI(data_labels, pred_cluster)\n",
        "            ari = ARI(data_labels, pred_cluster)\n",
        "            print('testing NMI, ARI, ACC at epoch %d is %f, %f, %f.' % (epoch, nmi, ari, acc))\n",
        "\n",
        "            if epoch % 5 == 0:  # save model at every 5 epochs\n",
        "                model_name = 'DAC_ep_' + str(epoch) + '.ckpt'\n",
        "                save_path = saver.save(sess, 'DAC_models/' + model_name)\n",
        "                print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "            epoch += 1\n",
        "\n",
        "elif mode == 'Testing':\n",
        "    test_batch_size = 64\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, \"DAC_models/DAC_ep_45.ckpt\")\n",
        "        print('model restored!')\n",
        "        all_predictions = np.zeros([len(image_label)], dtype=np.float32)\n",
        "        for i in range(len(image_datsa) // test_batch_size):\n",
        "            data_samples = get_batch_test(test_batch_size, image_data, i)\n",
        "            feed_dict = {image_pool_input: data_samples}\n",
        "            pred_cluster = sess.run(pred_label, feed_dict=feed_dict)\n",
        "            all_predictions[i * test_batch_size:(i + 1) * test_batch_size] = pred_cluster\n",
        "\n",
        "        acc = clustering_acc(image_label.astype(int), all_predictions.astype(int))\n",
        "        nmi = NMI(image_label.astype(int), all_predictions.astype(int))\n",
        "        ari = ARI(image_label.astype(int), all_predictions.astype(int))\n",
        "        print('testing NMI, ARI, ACC are %f, %f, %f.' % (nmi, ari, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}