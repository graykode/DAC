{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "GTKZfing21yp",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import PIL.Image as img\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Option\n",
    "mode = 'Training'\n",
    "num_cluster = 10\n",
    "eps = 1e-10\n",
    "height = 28\n",
    "width = 28\n",
    "channel = 1\n",
    "\n",
    "# Get Datas and Labels as batch_size\n",
    "def get_batch(batch_size, img_data, imgt_labels):\n",
    "    batch_index = random.sample(range(len(imgt_labels)), batch_size)\n",
    "\n",
    "    batch_data = np.empty([batch_size, height, width, channel], dtype=np.float32)\n",
    "    batch_label = np.empty([batch_size], dtype=np.int32)\n",
    "\n",
    "    for n, i in enumerate(batch_index):\n",
    "        batch_data[n, ...] = img_data[i, ...]\n",
    "        batch_label[n] = imgt_labels[i]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "# Get Datas and Labels as batch_size for Testing\n",
    "def get_batch_test(batch_size, img_data, i):\n",
    "    batch_data = np.copy(img_data[batch_size * i:batch_size * (i + 1), ...])\n",
    "    return batch_data\n",
    "\n",
    "def clustering_acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "\n",
    "def NMI(y_true, y_pred):\n",
    "    return metrics.normalized_mutual_info_score(y_true, y_pred)\n",
    "\n",
    "def ARI(y_true, y_pred):\n",
    "    return metrics.adjusted_rand_score(y_true, y_pred)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def ConvNetwork(in_img, num_cluster, name='ConvNetwork', reuse=False):\n",
    "\twith tf.variable_scope(name, reuse=reuse):\n",
    "\t\t# conv1\n",
    "\t\tconv1 = tf.layers.conv2d(in_img, 64, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "\t\tconv1 = tf.layers.batch_normalization(conv1, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tconv1 = tf.nn.relu(conv1)\n",
    "\t\t# conv2\n",
    "\t\tconv2 = tf.layers.conv2d(conv1, 64, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "\t\tconv2 = tf.layers.batch_normalization(conv2, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tconv2 = tf.nn.relu(conv2)\n",
    "\t\t# conv3\n",
    "\t\tconv3 = tf.layers.conv2d(conv2, 64, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "\t\tconv3 = tf.layers.batch_normalization(conv3, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tconv3 = tf.nn.relu(conv3)\n",
    "\t\tconv3 = tf.layers.max_pooling2d(conv3, [2,2], [2,2])\n",
    "\t\tconv3 = tf.layers.batch_normalization(conv3, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\t# conv4\n",
    "\t\tconv4 = tf.layers.conv2d(conv3, 128, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "\t\tconv4 = tf.layers.batch_normalization(conv4, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tconv4 = tf.nn.relu(conv4)\n",
    "\t\t# conv5\n",
    "\t\tconv5 = tf.layers.conv2d(conv4, 128, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "\t\tconv5 = tf.layers.batch_normalization(conv5, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tconv5 = tf.nn.relu(conv5)\n",
    "\t\t# conv6\n",
    "\t\tconv6 = tf.layers.conv2d(conv5, 128, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "\t\tconv6 = tf.layers.batch_normalization(conv6, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tconv6 = tf.nn.relu(conv6)\n",
    "\t\tconv6 = tf.layers.max_pooling2d(conv6, [2,2], [2,2])\n",
    "\t\tconv6 = tf.layers.batch_normalization(conv6, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\t# conv7\n",
    "\t\tconv7 = tf.layers.conv2d(conv6, 10, [1,1], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "\t\tconv7 = tf.layers.batch_normalization(conv7, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tconv7 = tf.nn.relu(conv7)\n",
    "\t\tconv7 = tf.layers.average_pooling2d(conv7, [2,2], [2,2])\n",
    "\t\tconv7 = tf.layers.batch_normalization(conv7, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tconv7_flat = tf.layers.flatten(conv7)\n",
    "\n",
    "\t\t# dense8\n",
    "\t\tfc8 = tf.layers.dense(conv7_flat, 10, kernel_initializer=tf.initializers.identity())\n",
    "\t\tfc8 = tf.layers.batch_normalization(fc8, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tfc8 = tf.nn.relu(fc8)\n",
    "\t\t# dense9\n",
    "\t\tfc9 = tf.layers.dense(fc8, num_cluster, kernel_initializer=tf.initializers.identity())\n",
    "\t\tfc9 = tf.layers.batch_normalization(fc9, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
    "\t\tfc9 = tf.nn.relu(fc9)\n",
    "\n",
    "\t\tout = tf.nn.softmax(fc9)\n",
    "\n",
    "\treturn out\n",
    "\n",
    "image_pool_input = tf.placeholder(shape=[None, height, width, channel], dtype=tf.float32, name='image_pool_input')\n",
    "u_thres = tf.placeholder(shape=[], dtype=tf.float32, name='u_thres')\n",
    "l_thres = tf.placeholder(shape=[], dtype=tf.float32, name='l_thres')\n",
    "lr = tf.placeholder(shape=[], dtype=tf.float32, name='learning_rate')\n",
    "\n",
    "label_feat = ConvNetwork(image_pool_input, num_cluster, name='ConvNetwork', reuse=False)\n",
    "label_feat_norm = tf.nn.l2_normalize(label_feat, dim=1)\n",
    "sim_mat = tf.matmul(label_feat_norm, label_feat_norm, transpose_b=True)\n",
    "\n",
    "pos_loc = tf.greater(sim_mat, u_thres, name='greater')\n",
    "neg_loc = tf.less(sim_mat, l_thres, name='less')\n",
    "pos_loc_mask = tf.cast(pos_loc, dtype=tf.float32)\n",
    "neg_loc_mask = tf.cast(neg_loc, dtype=tf.float32)\n",
    "\n",
    "pred_label = tf.argmax(label_feat, axis=1)\n",
    "\n",
    "# Deep Adaptive Image Clustering Cost Function Optimize\n",
    "pos_entropy = tf.multiply(-tf.log(tf.clip_by_value(sim_mat, eps, 1.0)), pos_loc_mask)\n",
    "neg_entropy = tf.multiply(-tf.log(tf.clip_by_value(1-sim_mat, eps, 1.0)), neg_loc_mask)\n",
    "\n",
    "loss_sum = tf.reduce_mean(pos_entropy) + tf.reduce_mean(neg_entropy)\n",
    "train_op = tf.train.RMSPropOptimizer(lr).minimize(loss_sum)\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST-data')  # your mnist data should be stored at 'MNIST-data'\n",
    "mnist_train = mnist.train.images\n",
    "mnist_train = np.reshape(mnist_train, (-1, 28, 28, 1))  # reshape into 1-channel image\n",
    "mnist_train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "mnist_test = mnist.test.images\n",
    "mnist_test = np.reshape(mnist_test, (-1, 28, 28, 1))  # reshape into 1-channel image\n",
    "mnist_test_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "image_data = np.concatenate([mnist_train, mnist_test], axis=0)\n",
    "image_label = np.concatenate([mnist_train_labels, mnist_test_labels], axis=0)\n",
    "\n",
    "print(len(image_data))\n",
    "\n",
    "# Compress Dimension\n",
    "mapping = {}\n",
    "mapped_label = 0\n",
    "for index,data in enumerate(image_label):\n",
    "  if(data in mapping):\n",
    "    image_label[index] = mapping[data]\n",
    "  else:\n",
    "    image_label[index] = mapping[data] = mapped_label+1\n",
    "    mapped_label += 1\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if mode == 'Training':\n",
    "    batch_size = 128\n",
    "    test_batch_size = 512\n",
    "    base_lr = 0.001\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        lamda = 0\n",
    "        epoch = 1\n",
    "        u = 0.95\n",
    "        l = 0.455\n",
    "\n",
    "        while u > l:\n",
    "            u = 0.95 - lamda\n",
    "            l = 0.455 + 0.1 * lamda\n",
    "            print(u, l)\n",
    "            for i in range(1, int(1001)):  # 1000 iterations is roughly 1 epoch\n",
    "                data_samples, _ = get_batch(batch_size, image_data, image_label)\n",
    "                feed_dict = {image_pool_input: data_samples, u_thres: u, l_thres: l, lr: base_lr}\n",
    "                train_loss, _ = sess.run([loss_sum, train_op], feed_dict=feed_dict)\n",
    "                if i % 20 == 0:\n",
    "                    print('training loss at iter %d is %f' % (i, train_loss))\n",
    "\n",
    "            lamda += 1.1 * 0.009\n",
    "            print(lamda)\n",
    "            # run testing every epoch\n",
    "            data_samples, data_labels = get_batch(test_batch_size, image_data, image_label)\n",
    "            feed_dict = {image_pool_input: data_samples}\n",
    "            pred_cluster = sess.run(pred_label, feed_dict=feed_dict)\n",
    "\n",
    "            acc = c(data_labels, pred_cluster)\n",
    "            nmi = NMI(data_labels, pred_cluster)\n",
    "            ari = ARI(data_labels, pred_cluster)\n",
    "            print('testing NMI, ARI, ACC at epoch %d is %f, %f, %f.' % (epoch, nmi, ari, acc))\n",
    "\n",
    "            if epoch % 5 == 0:  # save model at every 5 epochs\n",
    "                model_name = 'DAC_ep_' + str(epoch) + '.ckpt'\n",
    "                save_path = saver.save(sess, 'DAC_models/' + model_name)\n",
    "                print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "elif mode == 'Testing':\n",
    "    test_batch_size = 1000\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"DAC_models/DAC_ep_45.ckpt\")\n",
    "        print('model restored!')\n",
    "        all_predictions = np.zeros([len(image_label)], dtype=np.float32)\n",
    "        for i in range(len(image_datsa) // test_batch_size):\n",
    "            data_samples = get_batch_test(test_batch_size, image_data, i)\n",
    "            feed_dict = {image_pool_input: data_samples}\n",
    "            pred_cluster = sess.run(pred_label, feed_dict=feed_dict)\n",
    "            all_predictions[i * test_batch_size:(i + 1) * test_batch_size] = pred_cluster\n",
    "\n",
    "        acc = clustering_acc(image_label.astype(int), all_predictions.astype(int))\n",
    "        nmi = NMI(image_label.astype(int), all_predictions.astype(int))\n",
    "        ari = ARI(image_label.astype(int), all_predictions.astype(int))\n",
    "        print('testing NMI, ARI, ACC are %f, %f, %f.' % (nmi, ari, acc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "version": "0.3.2",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
